"""SQL metrics evaluator module."""

import logging
import time
from typing import Any, Dict, List, Optional, Tuple, Union

from sql_metrics_evaluator.src.database import DatabaseExecutor
from sql_metrics_evaluator.src.models import (
    EvaluationRequest,
    EvaluationResponse,
    QueryComplexity,
    SQLMetrics,
)
from sql_metrics_evaluator.src.parser import SQLParser

logger = logging.getLogger(__name__)


class SQLMetricsEvaluator:
    """Class for evaluating SQL generation models with real-time metrics."""

    def __init__(
        self,
        db_connection_string: Optional[str] = None,
        execution_timeout: int = 5000,
        complexity_weights: Optional[Dict[str, float]] = None,
    ) -> None:
        """Initialize the SQL metrics evaluator.

        Args:
            db_connection_string: Database connection string for execution accuracy testing
            execution_timeout: Query execution timeout in milliseconds
            complexity_weights: Weights for different complexity levels
        """
        self.parser = SQLParser()
        self.db_executor = None
        
        if db_connection_string:
            try:
                self.db_executor = DatabaseExecutor(
                    connection_string=db_connection_string,
                    timeout=execution_timeout
                )
                logger.info("Database executor initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize database executor: {str(e)}")
        
        # Default complexity weights
        self.complexity_weights = complexity_weights or {
            QueryComplexity.SIMPLE: 0.2,
            QueryComplexity.MEDIUM: 0.3,
            QueryComplexity.COMPLEX: 0.5,
        }

    def evaluate(
        self,
        generated_query: str,
        reference_query: str,
        query_complexity: Union[str, QueryComplexity] = QueryComplexity.MEDIUM,
        inference_latency: Optional[float] = None,
        database_schema: Optional[str] = None,
        execution_timeout: Optional[int] = None,
    ) -> SQLMetrics:
        """Evaluate a generated SQL query against a reference query.

        Args:
            generated_query: SQL query generated by the model
            reference_query: Reference SQL query to compare against
            query_complexity: Complexity level of the query
            inference_latency: Time taken to generate the query in milliseconds
            database_schema: Database schema for zero-shot evaluation
            execution_timeout: Timeout for query execution in milliseconds

        Returns:
            SQLMetrics object with evaluation results
        """
        start_time = time.time()
        
        # Initialize metrics
        metrics = SQLMetrics()
        
        # Set inference latency if provided
        if inference_latency is not None:
            metrics.inference_latency = inference_latency
        
        # Update database executor timeout if provided
        if execution_timeout is not None and self.db_executor:
            self.db_executor.timeout_ms = execution_timeout
        
        # Ensure query_complexity is a QueryComplexity enum
        if isinstance(query_complexity, str):
            try:
                query_complexity = QueryComplexity(query_complexity)
            except ValueError:
                query_complexity = QueryComplexity.MEDIUM
        
        # Calculate exact match accuracy
        exact_match = self._calculate_exact_match_accuracy(generated_query, reference_query)
        metrics.exact_match_accuracy = 1.0 if exact_match else 0.0
        
        # Calculate logical form accuracy
        logical_equivalence, comparison_details = self._calculate_logical_form_accuracy(
            generated_query, reference_query
        )
        metrics.logical_form_accuracy = 1.0 if logical_equivalence else 0.0
        metrics.parsing_details = comparison_details
        
        # Calculate execution accuracy if database executor is available
        if self.db_executor:
            execution_accuracy, execution_details = self._calculate_execution_accuracy(
                generated_query, reference_query
            )
            metrics.execution_accuracy = execution_accuracy
            metrics.execution_details = execution_details
        else:
            metrics.execution_accuracy = 0.0
            metrics.execution_details = {"error": "Database executor not available"}
        
        # Calculate complexity handling score
        metrics.complexity_handling = self._calculate_complexity_handling(
            generated_query, reference_query, query_complexity
        )
        
        # Calculate zero-shot performance if database schema is provided
        if database_schema:
            metrics.zero_shot_performance = self._calculate_zero_shot_performance(
                generated_query, reference_query, database_schema
            )
        
        # Record evaluation time
        metrics.evaluation_time = (time.time() - start_time) * 1000
        
        return metrics

    def evaluate_batch(
        self,
        requests: List[EvaluationRequest]
    ) -> List[EvaluationResponse]:
        """Evaluate a batch of SQL queries.

        Args:
            requests: List of evaluation requests

        Returns:
            List of evaluation responses
        """
        responses = []
        
        for request in requests:
            start_time = time.time()
            
            metrics = self.evaluate(
                generated_query=request.generated_query,
                reference_query=request.reference_query,
                query_complexity=request.query_complexity,
                database_schema=request.database_schema,
                execution_timeout=request.execution_timeout
            )
            
            evaluation_time = (time.time() - start_time) * 1000
            
            response = EvaluationResponse(
                metrics=metrics,
                generated_query=request.generated_query,
                reference_query=request.reference_query,
                query_complexity=request.query_complexity,
                evaluation_time=evaluation_time
            )
            
            responses.append(response)
        
        return responses

    def _calculate_exact_match_accuracy(self, generated_query: str, reference_query: str) -> bool:
        """Calculate exact match accuracy.

        Args:
            generated_query: Generated SQL query
            reference_query: Reference SQL query

        Returns:
            Boolean indicating exact match
        """
        # Normalize queries before comparison
        normalized_generated = self.parser.normalize_query(generated_query)
        normalized_reference = self.parser.normalize_query(reference_query)
        
        return normalized_generated == normalized_reference

    def _calculate_logical_form_accuracy(
        self, generated_query: str, reference_query: str
    ) -> Tuple[bool, Dict[str, Any]]:
        """Calculate logical form accuracy.

        Args:
            generated_query: Generated SQL query
            reference_query: Reference SQL query

        Returns:
            Tuple containing:
                - Boolean indicating logical equivalence
                - Dictionary with comparison details
        """
        # If database executor is available, use execution-based comparison
        if self.db_executor:
            return self.parser.try_logical_equivalence_with_execution(
                generated_query, reference_query, self.db_executor
            )
        
        # Otherwise, use static analysis
        comparison = self.parser.compare_queries(generated_query, reference_query)
        return comparison["logical_equivalence"], comparison

    def _calculate_execution_accuracy(
        self, generated_query: str, reference_query: str
    ) -> Tuple[float, Dict[str, Any]]:
        """Calculate execution accuracy.

        Args:
            generated_query: Generated SQL query
            reference_query: Reference SQL query

        Returns:
            Tuple containing:
                - Execution accuracy score (0.0 to 1.0)
                - Dictionary with execution details
        """
        if not self.db_executor:
            return 0.0, {"error": "Database executor not available"}
        
        # Execute both queries and compare results
        match, comparison = self.db_executor.compare_query_results(generated_query, reference_query)
        
        # If there was an error executing either query
        if not comparison["both_succeeded"]:
            if not comparison["query1_success"]:
                return 0.0, {
                    "error": "Generated query execution failed",
                    "details": comparison
                }
            else:
                return 0.0, {
                    "error": "Reference query execution failed",
                    "details": comparison
                }
        
        # If both queries executed successfully
        if match:
            return 1.0, {"match": True, "details": comparison}
        else:
            return 0.0, {"match": False, "details": comparison}

    def _calculate_complexity_handling(
        self, generated_query: str, reference_query: str, complexity: QueryComplexity
    ) -> float:
        """Calculate complexity handling score.

        Args:
            generated_query: Generated SQL query
            reference_query: Reference SQL query
            complexity: Query complexity level

        Returns:
            Complexity handling score (0.0 to 1.0)
        """
        # Parse both queries
        parsed_generated = self.parser.parse_query(generated_query)
        parsed_reference = self.parser.parse_query(reference_query)
        
        # If parsing failed for either query
        if not parsed_generated["success"] or not parsed_reference["success"]:
            return 0.0
        
        # Calculate component-level scores
        component_scores = {
            "tables": 1.0 if parsed_generated["tables"] == parsed_reference["tables"] else 0.0,
            "columns": 1.0 if parsed_generated["columns"] == parsed_reference["columns"] else 0.0,
            "joins": 1.0 if len(parsed_generated["joins"]) == len(parsed_reference["joins"]) else 0.0,
            "where": 1.0 if len(parsed_generated["where_conditions"]) == len(parsed_reference["where_conditions"]) else 0.0,
            "group_by": 1.0 if len(parsed_generated["group_by"]) == len(parsed_reference["group_by"]) else 0.0,
            "order_by": 1.0 if len(parsed_generated["order_by"]) == len(parsed_reference["order_by"]) else 0.0,
            "aggregations": 1.0 if parsed_generated["aggregations"] == parsed_reference["aggregations"] else 0.0
        }
        
        # Weight components based on complexity
        if complexity == QueryComplexity.SIMPLE:
            weights = {
                "tables": 0.3,
                "columns": 0.3,
                "joins": 0.1,
                "where": 0.2,
                "group_by": 0.05,
                "order_by": 0.05,
                "aggregations": 0.0
            }
        elif complexity == QueryComplexity.MEDIUM:
            weights = {
                "tables": 0.2,
                "columns": 0.2,
                "joins": 0.2,
                "where": 0.2,
                "group_by": 0.1,
                "order_by": 0.05,
                "aggregations": 0.05
            }
        else:  # COMPLEX
            weights = {
                "tables": 0.15,
                "columns": 0.15,
                "joins": 0.2,
                "where": 0.15,
                "group_by": 0.15,
                "order_by": 0.1,
                "aggregations": 0.1
            }
        
        # Calculate weighted score
        weighted_score = sum(component_scores[component] * weights[component] for component in component_scores)
        
        return weighted_score

    def _calculate_zero_shot_performance(
        self, generated_query: str, reference_query: str, database_schema: str
    ) -> float:
        """Calculate zero-shot performance score.

        Args:
            generated_query: Generated SQL query
            reference_query: Reference SQL query
            database_schema: Database schema description

        Returns:
            Zero-shot performance score (0.0 to 1.0)
        """
        # This is a simplified implementation
        # In a real-world scenario, you would:
        # 1. Parse the database schema
        # 2. Check if the generated query uses correct tables and columns from the schema
        # 3. Verify joins are valid based on the schema
        # 4. Ensure data types are used correctly
        
        # For now, we'll use a simplified approach based on logical form accuracy
        logical_equivalence, _ = self._calculate_logical_form_accuracy(generated_query, reference_query)
        
        if logical_equivalence:
            return 1.0
        
        # If not logically equivalent, check component-level matches
        parsed_generated = self.parser.parse_query(generated_query)
        parsed_reference = self.parser.parse_query(reference_query)
        
        if not parsed_generated["success"] or not parsed_reference["success"]:
            return 0.0
        
        # Calculate partial scores
        table_score = len(parsed_generated["tables"].intersection(parsed_reference["tables"])) / max(
            len(parsed_reference["tables"]), 1
        )
        
        column_score = len(parsed_generated["columns"].intersection(parsed_reference["columns"])) / max(
            len(parsed_reference["columns"]), 1
        )
        
        # Simplified zero-shot score
        zero_shot_score = 0.5 * table_score + 0.5 * column_score
        
        return zero_shot_score 