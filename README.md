# LLM Comparison with SQL Metrics Evaluator

This project integrates the LLM comparison app with the SQL metrics evaluator to provide real-time metrics for SQL queries generated by different LLM models.

## Components

1. **LLM Comparison App**: A React application for comparing different LLM models' SQL generation capabilities.
2. **SQL Metrics Evaluator**: A Python API for evaluating SQL queries with real-time metrics.

## Features

- Compare multiple LLM models side-by-side
- Visualize performance metrics with interactive charts
- Real-time SQL quality evaluation
- Advanced SQL metrics from the SQL metrics evaluator

## Getting Started

### Prerequisites

- Docker and Docker Compose
- OpenAI API key (optional, can use mock responses)

### Running the Application

Use the provided script to run the application:

```bash
./run.sh
```

By default, the application runs in development mode with mock responses. To use real API responses, provide your OpenAI API key:

```bash
./run.sh -k your_openai_api_key -m false
```

For production mode:

```bash
./run.sh -p
```

For more options:

```bash
./run.sh --help
```

## Project Structure

- `llm-comparison-app/`: React application for comparing LLM models
- `sql_metrics_evaluator/`: Python API for evaluating SQL queries
- `docker-compose.yml`: Docker Compose configuration for development
- `docker-compose.prod.yml`: Docker Compose configuration for production
- `run.sh`: Script to run the application

## Documentation

- [LLM Comparison App README](llm-comparison-app/README.md)
- [SQL Metrics Evaluator README](sql_metrics_evaluator/README.md)

## License

MIT 